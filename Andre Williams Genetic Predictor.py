# -*- coding: utf-8 -*-
"""AndreWilliamsDataAnalysisAssignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ao3YcPvtkV_nX503wJ7mYN_EXquXNLp_
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
# %matplotlib inline

"""# Pre-Deliverable

1.   Import File
2.   Clean Data
3.   Create Functions
"""

# Reading input csv file
filename = '/content/TrainingData_N183_p10.csv' 
data     = pd.read_csv(filename)
print(data.head())
print(data.shape)

# Creating numpy arrays for training features X and target labels y
X_train = np.array(data.iloc[:,:-1])
y_train = np.array(data.iloc[:,-1])
print(X_train.shape)
print(y_train.shape)
print(sorted(np.unique(y_train)))
final_catagory = sorted(np.unique(y_train))

# defining function to encode target label values to integers
# mapping ['African' 'EastAsian' 'European' 'NativeAmerican' 'Oceanian'] values in y_train to [0,1,2,3,4]

target_names = sorted(list(np.unique(y_train)))
for i in range(len(y_train)) :
    val = y_train[i]
    index = target_names.index(val)
    y_train[i] = index 

print(np.unique(y_train))

# Defining functions for standardizing input features X (a numpy array)
# Dimension of X is assumed to be (num_samples, num_features)

# Function for standardizing X 
def standardize_X(X) :
    mean_X = np.mean(X, axis=0)
    mean_X = mean_X.reshape([1,-1]) # reshaping mean_X to dim (1,9) , useful for broadcasting in following steps
    X_p    = X - mean_X 
    std_X  = np.std(X_p, axis=0) # computing standard deviation 
    std_X  = std_X.reshape([1,-1]) # reshaping std_X to dim (1,9) , useful for broadcasting in following steps
    X_p    = X_p / std_X 
    return X_p, mean_X, std_X

# Standardizing input features X_train
X_train, mean_X, std_X = standardize_X(X_train)
print('Mean_X' , mean_X)
print('STD_X'  , std_X)

# Defining a function for adding a feature with constant value of 1 to all X_train samples (for representing bias term)
# Input is feature array X (of shape num_samples x num_features)
# Output is augmented feature array aug_X (with column of 1s added to X)
def get_Xaugmented(X):
    arr_ones = np.ones(shape=(X.shape[0],1), dtype="float64")
    X_aug = np.concatenate(( arr_ones , X ) , axis=1) # _aug for augmented X
    return X_aug

# X_train_aug = get_Xaugmented(X_train)
# print(X_train.shape)
# print(X_train_aug.shape)
# print(X_train[0])
# print(X_train_aug[0])

# Defining function for performing batch gradient for Logisitc (multinomial) regression with Ridge(L2) regularization

# Input parameters
# X - data features
# y - target labels encoded as integers
# epochs - number of epochs to run batch gradient descent 
# lr - learning rate (alpha)
# lmbda - tuning parameter (multiplies the L2 regularisation term in the cost function)

# Return values
# B_reg_coeffs - learned parameters / regression coefficients as numpy array with shape (p+1) x K 
#                p - number of features in X
#                K - number of output classes/labels
def perform_multi_log_reg(X, y, epochs, lr, lmbda ) :
    num_samples  = X.shape[0]
    num_features = X.shape[1]
    num_classes  = len(np.unique(y_train)) # get number of classes by counting unique values

    # adding a column of 1s to X
    X_aug = get_Xaugmented(X)
    
    # creating Y array 
    Y     = np.zeros((num_samples, num_classes))
    for i in range(len(y)):
        index = np.squeeze(y[i])
        Y[i,index] = 1
    
    # Randomnly initializing B_reg_coeffs array 
    B_reg_coeffs = np.random.uniform(low=-1, high=1, size=(num_features+1 , num_classes))    
    
    for i in range(epochs):
    
        # Computing exp_XB array representing unnormalized class probabilities
        XB = np.matmul(X_aug, B_reg_coeffs)
        exp_XB = np.exp(XB)
    
        # Computing P array representing normalized class probabilities 
        sum_exp_XB = np.sum(exp_XB, axis=1, keepdims=True)
        P = exp_XB/sum_exp_XB
    
        # Computing Z matrix for ease of subsequent computations
        Z = np.zeros_like(B_reg_coeffs)
        Z[0,:] = B_reg_coeffs[0,:]
    
        # Updating B_reg_coeffs matrix 
        B_reg_coeffs = B_reg_coeffs + lr*( np.matmul( X_aug.T , Y - P ) - 2*lmbda*(B_reg_coeffs - Z) )
        
        #if (i%1000 == 0) or (i == epochs-1):
            #print("{} epochs completed".format(i+1))
    
    return B_reg_coeffs

# testing perform_multi_log_reg function
#B_reg_coeffs = perform_multi_log_reg(X_train, y_train, epochs=10000, lr=1e-5, lmbda=1e-2 )
#print(B_reg_coeffs.shape)

"""# Deliverable 1
Create Plot
"""

# Finding learned regression coefficients for each of lmbda values

# defining values for tuning parameter lambda 
lmbdas     = np.array([1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4])
log_lmbdas = np.log10(lmbdas)

# Defining learning rate alpha 
learning_rate = 1e-5 

# creating a list for storing B_reg_coeffs learned for each of the lmbda values 
list_B_reg_coeffs = []
for lmbda in lmbdas :
    B_reg_coeffs = perform_multi_log_reg(X_train, y_train, epochs=10000, lr=learning_rate, lmbda=lmbda )
    list_B_reg_coeffs.append(B_reg_coeffs)

# converting list list_B_reg_coeffs to numpy array
arr_B_reg_coeffs = np.array(list_B_reg_coeffs)
print(arr_B_reg_coeffs.shape) # array of shape num_lmbdas x (num_features+1) x num_classes 

#print(len(list_B_reg_coeffs))
#print(list_B_reg_coeffs[0].shape)

# Generating plots 

num_features = X_train.shape[1]
num_classes  = len(target_names)
num_lmbdas   = len(lmbdas)

for i in range(num_classes) :
    plt.figure(figsize=(15,5))
    plt.title("Regression coeffs for {} (K={}) class".format(target_names[i], i))
    plt.xlabel("log(lambda)")
    plt.ylabel("Regression Coeffs")
    for j in range(1, num_features+1) :
        coeffs = arr_B_reg_coeffs[:,j,i]
        plt.plot(log_lmbdas , coeffs, label="j={}".format(j))
    plt.legend(loc="lower right")

# OPTIONAL 
# Testing classification accuracy on the training set with reg coefficients for lambda 1e-1
# reg_coeffs = arr_B_reg_coeffs[3] # corresponding to lambda of 1e-1
# X_train_aug = get_Xaugmented(X_train)
# y_pred = np.argmax(np.matmul(X_train_aug, reg_coeffs), axis=1)
# accuracy = np.sum(y_pred == y_train)/len(y_train)
# print(accuracy)

"""# Deliverable 2 - Effect of the tuning parameter lambda on the 5 fold cross validation error

"""

# This function is used for computing the categorical cross entropy loss
# Input parameters : 
# y_true       - ground truth label values encoded as integers 
# y_pred_probs - prediction probabilities for each of the output classes 
# Return values :
# crossentropy_loss (single/scalar value)
def compute_categorical_crossentropy_loss(y_true, y_pred_probs) :

#     Optional for computing and printing classification accuracy    
#     y_pred_class = np.argmax(y_pred_probs, axis=1)
#     accuracy = np.sum(y_pred_class == y_true)/len(y_true)
#     print("Accuracy {}".format(accuracy))
    
    num_samples = y_true.shape[0]
    num_output_classes = y_pred_probs.shape[1]
    log_losses = []
    for i in range(num_samples) :
        pred_prob_true_class     = y_pred_probs[i,y_true[i]]
        log_loss = -np.log10(pred_prob_true_class)
        log_losses.append(log_loss)
    crossentropy_loss = np.mean(log_losses)   
    return crossentropy_loss

# This function is used for computing the prediction probabilities 
# Input parameters :
# X - input features values for which output class probabilities need to be predicted (numpy array of shape num_samples * num_features)
#   - X must be normalized (mean zero and variance unity) before passing to this function
#   - This function with also augment X with a column of 1s. So, X should not be augmented before passing to this function 
# B_reg_coeffs - learned parameters 
# Returns :
# y_pred_probs - output prediction probabilities for each of the classes. numpy array of shape num_samples * num_classes 
def compute_prediction_probs( X , B_reg_coeffs ) :
    X_aug = get_Xaugmented(X)
    output_scores = np.matmul(X_aug, B_reg_coeffs)
    exp_output_scores = np.exp(output_scores)
    sum_exp_output_scores = np.sum(exp_output_scores, axis=1, keepdims=True)
    y_pred_probs = exp_output_scores / sum_exp_output_scores
    return y_pred_probs

lmbdas     = np.array([1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4])
log_lmbdas = np.log10(lmbdas)
#print(log_lmbda)

# validation data start and end indices for 5-fold cross validation
num_samples = X_train.shape[0]
fold_factor = 5 
num_samples_val = num_samples//fold_factor
s_indices = [ num_samples_val*ind for ind in range(fold_factor)]
e_indices = [ ind + num_samples_val for ind in s_indices ]
# print(s_indices)
# print(e_indices)

# Randomnly shuffling values in X_train and y_train 
np.random.seed(0) # to make results reproducible
indices = np.random.permutation(X_train.shape[0])
X_train_shuffled = X_train[indices,:]
y_train_shuffled = y_train[indices]

learning_rate = 1e-5
CV_errors = [] # initializing empty list for storing the cross validation errors for each of the tuning parameter lmbda values     
for i, lmbda in enumerate(lmbdas) :
    print('Computing 5-fold cross validation error for tuning parameter={}'.format(lmbda))
    categorical_crossentropy_losses = [] 
    for i in range(fold_factor) :
        s_index = s_indices[i]
        e_index = e_indices[i]
        X_val_split   = X_train_shuffled[s_index:e_index, :]
        y_val_split   = y_train_shuffled[s_index:e_index]
        X_train_split = np.vstack(( X_train_shuffled[0:s_index,:] , X_train_shuffled[e_index:, :] ))
        y_train_split = np.concatenate(( y_train_shuffled[0:s_index] , y_train_shuffled[e_index:] ))
    
        # Preprocessing (normalizing for 0 mean and unit variance on the train split) X_train_split and X_val_split
        X_train_split_p, mean_X_train_split, std_X_train_split = standardize_X(X_train_split)
        X_val_split_p = (X_val_split - mean_X_train_split)/std_X_train_split
        
        # Performing multinomial logistic regression and computing categorical cross entropy losses on the validation set
        B_reg_coeffs = perform_multi_log_reg(X_train_split_p, y_train_split, epochs=10000, lr=learning_rate, lmbda=lmbda ) # B_reg_coeffs - learned parameters / regression coefficients as numpy array with shape (p+1) x K 
        
        # Computing predictions probabilities on the validation split 
        y_val_split_probs = compute_prediction_probs( X_val_split_p , B_reg_coeffs )
        
        # Computing categorical cross entropy loss on the validation set
        crossentropy_loss = compute_categorical_crossentropy_loss(y_val_split, y_val_split_probs)

        categorical_crossentropy_losses.append(crossentropy_loss)
    
    CV_error = np.mean(categorical_crossentropy_losses)
    CV_errors.append(CV_error)    
    print("CV_error", CV_error)

plt.figure()
plt.xlabel('log10(lambda)')
plt.ylabel('Cross Validation Error')
plt.title('Effect of tuning parameter on the Cross Validation Error')
plt.plot(log_lmbdas, CV_errors)
plt.rcParams['figure.figsize'] = [10, 10]
plt.show()

"""**0.01 is the best Lambda Value based on Categorical Cross Entropy**

# Deliverable 4 - Retrain model with optimal lambda predict test set
"""

optimal = perform_multi_log_reg(X_train, y_train, epochs=10000, lr=learning_rate, lmbda=0.01 )

filename = '/content/TestData_N111_p10.csv' 
data     = pd.read_csv(filename)
X_test = np.array(data.iloc[:,:-1])
y_test = np.array(data.iloc[:,-1])
print(X_test.shape)
print(y_test.shape)
print(sorted(np.unique(y_test)))

#Standardize features for the test set
X_test, mean_X, std_X = standardize_X(X_test)

Yhat = compute_prediction_probs(X_test,optimal)
print(Yhat)

most_probable = np.argmax(Yhat, axis=0)
print(most_probable)
print(final_catagory[most_probable.argmax(axis=0)])

"""# Deliverable 5 - How do Class Label probabilities differ

African Ancestry has the highest probability (for african americans); however, the unknown Ancestry has higher probability than Mexican Ancestry because Mexico was not a Class Set in our Training Data.

# **Extra Credit**

# Deliverable 1 using sklearn
"""

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

# Reading input csv file
filename = '/content/TrainingData_N183_p10.csv' 
data     = pd.read_csv(filename)

# Creating numpy arrays for training features X and target labels y
X_train = np.array(data.iloc[:,:-1])
y_train = np.array(data.iloc[:,-1])

# Converting target label names to integers between 0 and num_classes-1
le = LabelEncoder()
le.fit(y_train)
#print(list(le.classes_))
target_names = list(le.classes_)
y_train = le.transform(y_train)
#print(y_train[:5])

# Standardizing input features X to have zero mean and unit variance (X is numpy array with shape num_samples x num_features )
scaler = StandardScaler()
scaler.fit(X_train)
mean_Xtrain = scaler.mean_
std_Xtrain  = scaler.scale_
# print(mean_Xtrain)
# print(std_Xtrain)
X_train = scaler.transform(X_train)

# Finding learned regression coefficients for each of lmbda values

# defining values for tuning parameter lambda 
lmbdas     = np.array([1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4])
log_lmbdas = np.log10(lmbdas)

# creating a list for storing B_reg_coeffs learned for each of the lmbda values 
list_B_reg_coeffs = []
for lmbda in lmbdas :
    model = LogisticRegression(penalty='l2' , C = 1/lmbda , random_state=0 , max_iter = 10000)
    model.fit(X_train, y_train)
    B_reg_coeffs = model.coef_ # numpy array with learned coefficient values of shape n_classes x n_features . Note that the bias or intercept term is not included    
    list_B_reg_coeffs.append(B_reg_coeffs)
    print("Lambda - {}. Model Score - {}".format(lmbda, model.score(X_train , y_train)))

# converting list list_B_reg_coeffs to numpy array
arr_B_reg_coeffs = np.array(list_B_reg_coeffs)
print(arr_B_reg_coeffs.shape)

# Generating plots 

num_features = X_train.shape[1]
num_classes  = len(target_names)
num_lmbdas   = len(lmbdas)

for i in range(num_classes) :
    plt.figure(figsize=(15,5))
    plt.title("Regression coeffs for {} (K={}) class".format(target_names[i], i))
    plt.xlabel("log(lambda)")
    plt.ylabel("Regression Coeffs")
    for j in range(num_features) :
        coeffs = arr_B_reg_coeffs[:,i,j]
        plt.plot(log_lmbdas , coeffs, label="j={}".format(j+1))
    plt.legend(loc="lower right")

"""# Deliverable 2 using sklearn"""

lmbdas     = np.array([1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4])
log_lmbdas = np.log10(lmbdas)
#print(log_lmbda)

# Performing 5 fold cross validation 
fold_factor = 5 

CV_errors = [] # initializing empty list for storing the cross validation errors for each of the tuning parameter lmbda values     
for i, lmbda in enumerate(lmbdas) :
    print('Computing 5-fold cross validation error for tuning parameter={}'.format(lmbda))
    categorical_crossentropy_losses = []
    model = LogisticRegression(penalty='l2' , C = 1/lmbda , random_state=0 , max_iter = 10000)
    scores = cross_val_score(model, X_train, y_train, cv=fold_factor, scoring='neg_log_loss')
    losses = -scores
    categorical_crossentropy_losses.append(losses)
    CV_error = np.mean(categorical_crossentropy_losses)
    CV_errors.append(CV_error)    
    print("CV_error", CV_error)

plt.figure()
plt.xlabel('log10(lambda)')
plt.ylabel('Cross Validation Error')
plt.title('Effect of tuning parameter on the Cross Validation Error')
plt.plot(log_lmbdas, CV_errors)
plt.rcParams['figure.figsize'] = [10, 10]
plt.show()

"""# Deliverable 3 using sklearn

Best tuning parameter using sklearn - 0.0001

# Deliverable 4 using sklearn
"""

# Finding learned regression coefficients for each of lmbda values

best_lmbda = 1e-4
# Fit model to entire training data
model = LogisticRegression(penalty='l2' , C = 1/best_lmbda , random_state=0 , max_iter = 10000)
model.fit(X_train, y_train)
B_reg_coeffs = model.coef_ # numpy array with learned coefficient values of shape n_classes x n_features . Note that the bias or intercept term is not included    
B_reg_intercept = model.intercept_

# Read inputs from csv test file 
test_filename = '/content/TestData_N111_p10.csv' 
test_data     = pd.read_csv(test_filename)

# Creating numpy array for training features X 
X_test = np.array(test_data.iloc[:,:-1])
#print(X_test.shape)

# Normalizing features in X_test as per StandardScalar scaler fitted on training data
X_test = scaler.transform(X_test)


y_pred_probs = model.predict_proba(X_test)
y_pred_labels = model.predict(X_test)
y_pred_label_names = le.inverse_transform(y_pred_labels)
#print(y_pred_probs.shape)
print(y_pred_probs[:5])
#print(y_pred_labels.shape)
print(y_pred_labels[:5])
#print(y_pred_label_names.shape)
print(y_pred_label_names[:5])